import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
from sklearn.decomposition import PCA, FactorAnalysis, TruncatedSVD, FastICA, KernelPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding
from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif, mutual_info_regression, RFE, SelectFromModel
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class DimensionalityReducer:
    """Comprehensive dimensionality reduction module"""
    
    @staticmethod
    def get_method_info():
        """Get detailed information about available methods"""
        return {
            # Linear Methods
            "PCA": {
                "type": "Linear, Unsupervised",
                "description": "Principal Component Analysis - Reduces dimensionality by finding principal components that maximize variance",
                "best_for": "Data with linear relationships, noise reduction, feature decorrelation",
                "pros": "Fast, interpretable, preserves global structure",
                "cons": "Linear assumptions, may lose non-linear patterns",
                "when_use": "High correlation between features, need interpretable components",
                "supervised": False,
                "category": "Linear Transformation"
            },
            "Factor Analysis": {
                "type": "Linear, Unsupervised",
                "description": "Models observed variables as linear combinations of latent factors",
                "best_for": "Understanding underlying factors, psychological/social research",
                "pros": "Identifies latent factors, handles noise well",
                "cons": "Assumes linear relationships, requires factor interpretation",
                "when_use": "Belief that data is generated by hidden factors",
                "supervised": False,
                "category": "Linear Transformation"
            },
            "LDA": {
                "type": "Linear, Supervised",
                "description": "Linear Discriminant Analysis - Finds linear combinations that best separate classes",
                "best_for": "Classification tasks, maximizing class separability",
                "pros": "Uses class information, excellent for classification preprocessing",
                "cons": "Requires labeled data, limited to (n_classes-1) components",
                "when_use": "Classification problems, need maximum class separation",
                "supervised": True,
                "category": "Linear Transformation"
            },
            "Truncated SVD": {
                "type": "Linear, Unsupervised",
                "description": "Singular Value Decomposition for sparse matrices",
                "best_for": "Text data, sparse matrices, LSA",
                "pros": "Works with sparse data, memory efficient",
                "cons": "May not preserve distances well",
                "when_use": "Large sparse datasets, text analysis",
                "supervised": False,
                "category": "Linear Transformation"
            },
            "ICA": {
                "type": "Linear, Unsupervised", 
                "description": "Independent Component Analysis - Separates multivariate signal into independent components",
                "best_for": "Signal separation, blind source separation",
                "pros": "Finds statistically independent components",
                "cons": "Assumes independence, sensitive to outliers",
                "when_use": "Signal processing, cocktail party problem",
                "supervised": False,
                "category": "Linear Transformation"
            },
            
            # Non-linear Methods
            "t-SNE": {
                "type": "Non-linear, Unsupervised",
                "description": "t-Distributed Stochastic Neighbor Embedding - Excellent for visualization",
                "best_for": "Data visualization, cluster discovery",
                "pros": "Excellent visualization, reveals hidden patterns",
                "cons": "Slow, not suitable for preprocessing ML models",
                "when_use": "Data exploration, visualization, cluster analysis",
                "supervised": False,
                "category": "Non-linear Manifold"
            },
            "Isomap": {
                "type": "Non-linear, Unsupervised",
                "description": "Isometric Mapping - Preserves geodesic distances on manifold",
                "best_for": "Non-linear manifold data, preserving geodesic distances",
                "pros": "Good for non-linear structures, preserves global geometry",
                "cons": "Sensitive to noise, requires manifold assumption",
                "when_use": "Data lies on non-linear manifold",
                "supervised": False,
                "category": "Non-linear Manifold"
            },
            "LLE": {
                "type": "Non-linear, Unsupervised",
                "description": "Locally Linear Embedding - Preserves local neighborhood structure",
                "best_for": "Non-linear dimensionality reduction, preserving local structure",
                "pros": "Preserves local relationships, good for manifold learning",
                "cons": "Sensitive to noise, may create holes in embedding",
                "when_use": "Data with strong local structure",
                "supervised": False,
                "category": "Non-linear Manifold"
            },
            "MDS": {
                "type": "Non-linear, Unsupervised",
                "description": "Multidimensional Scaling - Preserves pairwise distances",
                "best_for": "Distance preservation, similarity analysis",
                "pros": "Preserves pairwise distances, intuitive",
                "cons": "Computationally expensive for large datasets",
                "when_use": "Distance/similarity relationships are important",
                "supervised": False,
                "category": "Non-linear Manifold"
            },
            "Spectral Embedding": {
                "type": "Non-linear, Unsupervised",
                "description": "Uses graph Laplacian eigenvectors for embedding",
                "best_for": "Graph-like data, clustering preprocessing",
                "pros": "Good for complex structures, clustering applications",
                "cons": "Requires parameter tuning, sensitive to noise",
                "when_use": "Graph-structured data, complex geometries",
                "supervised": False,
                "category": "Non-linear Manifold"
            },
            "Kernel PCA": {
                "type": "Non-linear, Unsupervised",
                "description": "PCA in high-dimensional feature space via kernel trick",
                "best_for": "Non-linear patterns, kernel methods preprocessing",
                "pros": "Captures non-linear patterns, various kernel options",
                "cons": "Parameter selection, computational complexity",
                "when_use": "Non-linear patterns, before SVM",
                "supervised": False,
                "category": "Non-linear Transformation"
            },
            
            # Feature Selection Methods
            "Univariate Selection": {
                "type": "Statistical, Supervised",
                "description": "Selects features based on univariate statistical tests",
                "best_for": "Quick feature filtering, statistical significance",
                "pros": "Fast, interpretable, good baseline",
                "cons": "Ignores feature interactions",
                "when_use": "Initial feature filtering, many irrelevant features",
                "supervised": True,
                "category": "Feature Selection"
            },
            "Recursive Feature Elimination": {
                "type": "Model-based, Supervised",
                "description": "Recursively eliminates features using model importance",
                "best_for": "Finding optimal feature subset, model-specific selection",
                "pros": "Considers feature interactions, model-specific",
                "cons": "Computationally expensive, model-dependent",
                "when_use": "Need optimal feature subset for specific model",
                "supervised": True,
                "category": "Feature Selection"
            },
            "Model-based Selection": {
                "type": "Model-based, Supervised",
                "description": "Selects features based on model importance scores",
                "best_for": "Tree-based models, importance-based selection",
                "pros": "Fast, uses model knowledge, handles interactions",
                "cons": "Model-dependent, may overfit",
                "when_use": "Tree-based models, quick importance-based filtering",
                "supervised": True,
                "category": "Feature Selection"
            }
        }
    
    @staticmethod
    def get_method_categories():
        """Get methods grouped by category"""
        methods = DimensionalityReducer.get_method_info()
        categories = {}
        for method, info in methods.items():
            category = info["category"]
            if category not in categories:
                categories[category] = []
            categories[category].append(method)
        return categories
    
    @staticmethod
    def analyze_dimensionality(df, target_col=None):
        """Analyze dataset dimensionality and provide recommendations"""
        features = [col for col in df.select_dtypes(include=[np.number]).columns if col != target_col]
        n_samples = len(df)
        n_features = len(features)
        
        analysis = {
            "n_samples": n_samples,
            "n_features": n_features,
            "ratio": n_samples / n_features if n_features > 0 else 0,
            "recommendations": []
        }
        
        # Add recommendations based on analysis
        if n_features < 2:
            analysis["recommendations"].append("❌ Minimal 2 fitur numerik diperlukan untuk reduksi dimensi")
        elif analysis["ratio"] < 10:
            analysis["recommendations"].append("⚠️ Rasio sampel/fitur rendah. Reduksi dimensi sangat direkomendasikan")
        elif n_features > 50:
            analysis["recommendations"].append("💡 Dataset berdimensi tinggi. Reduksi dimensi dapat meningkatkan performa")
        else:
            analysis["recommendations"].append("✅ Dimensionalitas dataset cukup baik")
        
        # Correlation analysis
        if len(features) > 1:
            corr_matrix = df[features].corr()
            high_corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if abs(corr_matrix.iloc[i, j]) > 0.7:
                        high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))
            
            if high_corr_pairs:
                analysis["recommendations"].append(f"🔍 {len(high_corr_pairs)} pasang fitur berkorelasi tinggi. PCA/Factor Analysis efektif")
                analysis["high_correlations"] = high_corr_pairs
            else:
                analysis["recommendations"].append("ℹ️ Korelasi antar fitur rendah. Manfaat metode linear mungkin terbatas")
        
        return analysis
    
    @staticmethod
    def perform_dimensionality_reduction(df, method, features, target_col=None, n_components=2, **params):
        """Perform dimensionality reduction with specified method"""
        X = df[features]
        y = df[target_col] if target_col and target_col in df.columns else None
        
        # Standardize data for most methods (except tree-based feature selection)
        if method not in ["Model-based Selection", "Recursive Feature Elimination"]:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
        else:
            X_scaled = X.values
            scaler = None
        
        model = None
        X_reduced = None
        feature_names = None
        additional_info = {}
        
        try:
            # Linear Methods
            if method == "PCA":
                model = PCA(n_components=n_components, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"PC{i+1}" for i in range(n_components)]
                additional_info = {
                    "explained_variance_ratio": model.explained_variance_ratio_,
                    "cumulative_variance": np.cumsum(model.explained_variance_ratio_),
                    "components": model.components_
                }
            
            elif method == "Factor Analysis":
                n_comp = min(n_components, len(features)-1)
                model = FactorAnalysis(n_components=n_comp, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"Factor{i+1}" for i in range(n_comp)]
                additional_info = {
                    "components": model.components_,
                    "noise_variance": model.noise_variance_
                }
            
            elif method == "LDA":
                if y is None:
                    raise ValueError("LDA requires target variable")
                n_classes = len(np.unique(y))
                n_comp = min(n_components, n_classes-1, len(features))
                model = LinearDiscriminantAnalysis(n_components=n_comp)
                X_reduced = model.fit_transform(X_scaled, y)
                feature_names = [f"LD{i+1}" for i in range(n_comp)]
                additional_info = {
                    "explained_variance_ratio": model.explained_variance_ratio_,
                    "scalings": model.scalings_
                }
            
            elif method == "Truncated SVD":
                model = TruncatedSVD(n_components=n_components, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"SVD{i+1}" for i in range(n_components)]
                additional_info = {
                    "explained_variance_ratio": model.explained_variance_ratio_,
                    "components": model.components_
                }
            
            elif method == "ICA":
                model = FastICA(n_components=n_components, random_state=42, max_iter=1000)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"IC{i+1}" for i in range(n_components)]
                additional_info = {
                    "components": model.components_,
                    "mixing_matrix": model.mixing_
                }
            
            # Non-linear Methods
            elif method == "t-SNE":
                perplexity = params.get('perplexity', 30)
                learning_rate = params.get('learning_rate', 200)
                model = TSNE(n_components=n_components, perplexity=perplexity, 
                           learning_rate=learning_rate, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"tSNE{i+1}" for i in range(n_components)]
                additional_info = {
                    "kl_divergence": model.kl_divergence_,
                    "n_iter": model.n_iter_
                }
            
            elif method == "Isomap":
                n_neighbors = params.get('n_neighbors', 5)
                model = Isomap(n_components=n_components, n_neighbors=n_neighbors)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"Iso{i+1}" for i in range(n_components)]
                additional_info = {
                    "reconstruction_error": model.reconstruction_error()
                }
            
            elif method == "LLE":
                n_neighbors = params.get('n_neighbors', 5)
                model = LocallyLinearEmbedding(n_components=n_components, n_neighbors=n_neighbors, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"LLE{i+1}" for i in range(n_components)]
                additional_info = {
                    "reconstruction_error": model.reconstruction_error_
                }
            
            elif method == "MDS":
                model = MDS(n_components=n_components, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"MDS{i+1}" for i in range(n_components)]
                additional_info = {
                    "stress": model.stress_
                }
            
            elif method == "Spectral Embedding":
                n_neighbors = params.get('n_neighbors', 5)
                model = SpectralEmbedding(n_components=n_components, n_neighbors=n_neighbors, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"SE{i+1}" for i in range(n_components)]
            
            elif method == "Kernel PCA":
                kernel = params.get('kernel', 'rbf')
                gamma = params.get('gamma', None)
                model = KernelPCA(n_components=n_components, kernel=kernel, gamma=gamma, random_state=42)
                X_reduced = model.fit_transform(X_scaled)
                feature_names = [f"KPC{i+1}" for i in range(n_components)]
            
            # Feature Selection Methods
            elif method == "Univariate Selection":
                k = min(n_components, len(features))
                if y is None:
                    raise ValueError("Univariate selection requires target variable")
                
                # Choose score function based on target type
                if pd.api.types.is_numeric_dtype(df[target_col]):
                    score_func = f_regression
                else:
                    score_func = f_classif
                
                model = SelectKBest(score_func=score_func, k=k)
                X_reduced = model.fit_transform(X, y)
                selected_features = [features[i] for i in model.get_support(indices=True)]
                feature_names = selected_features
                additional_info = {
                    "scores": model.scores_,
                    "pvalues": model.pvalues_,
                    "selected_features": selected_features
                }
            
            elif method == "Recursive Feature Elimination":
                k = min(n_components, len(features))
                if y is None:
                    raise ValueError("RFE requires target variable")
                
                # Use appropriate estimator
                if pd.api.types.is_numeric_dtype(df[target_col]):
                    estimator = RandomForestRegressor(n_estimators=100, random_state=42)
                else:
                    estimator = RandomForestClassifier(n_estimators=100, random_state=42)
                
                model = RFE(estimator=estimator, n_features_to_select=k)
                X_reduced = model.fit_transform(X, y)
                selected_features = [features[i] for i in model.get_support(indices=True)]
                feature_names = selected_features
                additional_info = {
                    "ranking": model.ranking_,
                    "selected_features": selected_features
                }
            
            elif method == "Model-based Selection":
                if y is None:
                    raise ValueError("Model-based selection requires target variable")
                
                # Use appropriate estimator
                if pd.api.types.is_numeric_dtype(df[target_col]):
                    estimator = RandomForestRegressor(n_estimators=100, random_state=42)
                else:
                    estimator = RandomForestClassifier(n_estimators=100, random_state=42)
                
                estimator.fit(X, y)
                threshold = params.get('threshold', 'mean')
                model = SelectFromModel(estimator, threshold=threshold, prefit=True)
                X_reduced = model.transform(X)
                selected_features = [features[i] for i in model.get_support(indices=True)]
                feature_names = selected_features
                additional_info = {
                    "feature_importances": estimator.feature_importances_,
                    "selected_features": selected_features,
                    "threshold_value": model.threshold_
                }
            
            return {
                "model": model,
                "X_reduced": X_reduced,
                "feature_names": feature_names,
                "scaler": scaler,
                "additional_info": additional_info,
                "success": True,
                "message": f"{method} berhasil dijalankan"
            }
            
        except Exception as e:
            return {
                "model": None,
                "X_reduced": None,
                "feature_names": None,
                "scaler": None,
                "additional_info": {},
                "success": False,
                "message": f"Error: {str(e)}"
            }
    
    @staticmethod
    def plot_reduction_results(X_reduced, y=None, method="", n_components=2, feature_names=None):
        """Plot dimensionality reduction results"""
        if X_reduced is None:
            return None
            
        fig, axes = plt.subplots(1, 1 if n_components <= 2 else 2, figsize=(15, 6))
        
        if n_components == 1:
            # Histogram for 1D
            sns.histplot(x=X_reduced[:, 0], hue=y, multiple="stack", 
                        palette="viridis", ax=axes if n_components <= 2 else axes[0])
            axes.set_xlabel(feature_names[0] if feature_names else "Component 1")
            axes.set_title(f"{method} - 1D Visualization")
            
        elif n_components == 2:
            # Scatter plot for 2D
            sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=y, 
                          palette="viridis", ax=axes if n_components <= 2 else axes[0],
                          legend='full' if y is not None else False, alpha=0.7)
            axes.set_xlabel(feature_names[0] if feature_names else "Component 1")
            axes.set_ylabel(feature_names[1] if feature_names else "Component 2")
            axes.set_title(f"{method} - 2D Visualization")
            
        elif n_components >= 3:
            # 2D projection for higher dimensions
            sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=y,
                          palette="viridis", ax=axes[0], legend='full' if y is not None else False, alpha=0.7)
            axes[0].set_xlabel(feature_names[0] if feature_names else "Component 1")
            axes[0].set_ylabel(feature_names[1] if feature_names else "Component 2")
            axes[0].set_title(f"{method} - Components 1 vs 2")
            
            # Another projection
            if X_reduced.shape[1] >= 3:
                sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 2], hue=y,
                              palette="viridis", ax=axes[1], legend='full' if y is not None else False, alpha=0.7)
                axes[1].set_xlabel(feature_names[0] if feature_names else "Component 1") 
                axes[1].set_ylabel(feature_names[2] if feature_names else "Component 3")
                axes[1].set_title(f"{method} - Components 1 vs 3")
        
        plt.tight_layout()
        return fig
    
    @staticmethod
    def plot_explained_variance(additional_info, method):
        """Plot explained variance for applicable methods"""
        if "explained_variance_ratio" not in additional_info:
            return None
            
        explained_var = additional_info["explained_variance_ratio"]
        cumulative_var = additional_info.get("cumulative_variance", np.cumsum(explained_var))
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Individual explained variance
        components = range(1, len(explained_var) + 1)
        ax1.bar(components, explained_var, alpha=0.7, color='skyblue')
        ax1.set_xlabel('Component')
        ax1.set_ylabel('Explained Variance Ratio')
        ax1.set_title(f'{method} - Individual Explained Variance')
        ax1.set_xticks(components)
        
        # Cumulative explained variance
        ax2.plot(components, cumulative_var, 'bo-', markersize=6)
        ax2.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80% Variance')
        ax2.axhline(y=0.95, color='g', linestyle='--', alpha=0.7, label='95% Variance')
        ax2.set_xlabel('Number of Components')
        ax2.set_ylabel('Cumulative Explained Variance Ratio')
        ax2.set_title(f'{method} - Cumulative Explained Variance')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_xticks(components)
        
        plt.tight_layout()
        return fig
    
    @staticmethod
    def plot_component_heatmap(additional_info, original_features, method):
        """Plot component heatmap for linear methods"""
        if "components" not in additional_info:
            return None
            
        components = additional_info["components"]
        n_components = components.shape[0]
        
        fig, ax = plt.subplots(figsize=(12, max(6, n_components * 0.5)))
        
        # Create heatmap
        sns.heatmap(components, 
                   xticklabels=original_features,
                   yticklabels=[f"Component {i+1}" for i in range(n_components)],
                   cmap='RdBu_r', center=0, annot=True, fmt='.3f', ax=ax)
        
        ax.set_title(f'{method} - Component Loadings')
        ax.set_xlabel('Original Features')
        ax.set_ylabel('Components')
        
        plt.tight_layout()
        return fig
